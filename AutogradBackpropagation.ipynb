{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3662277b",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "\n",
    "Torch Autograd is PyTorch's automatic differentiation engine that powers neural network training by computing gradients of tensor operations. It enables the easy implementation of backpropagation for optimizing complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "712e3261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89088325",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m z.retain_grad() \n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Compute the gradients\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m z_sum = \u001b[43mz\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGradient of x: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.grad\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGradient of y: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my.grad\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/PyTorch/.venv/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    513\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    514\u001b[39m         Tensor.backward,\n\u001b[32m    515\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    520\u001b[39m         inputs=inputs,\n\u001b[32m    521\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/PyTorch/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:259\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    250\u001b[39m inputs = (\n\u001b[32m    251\u001b[39m     (inputs,)\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch.Tensor, graph.GradientEdge))\n\u001b[32m   (...)\u001b[39m\u001b[32m    255\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[32m    256\u001b[39m )\n\u001b[32m    258\u001b[39m grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m grad_tensors_ = \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    261\u001b[39m     retain_graph = create_graph\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/PyTorch/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:132\u001b[39m, in \u001b[36m_make_grads\u001b[39m\u001b[34m(outputs, grads, is_grads_batched)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out.requires_grad:\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m out.numel() != \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    133\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         )\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out.dtype.is_floating_point:\n\u001b[32m    136\u001b[39m         msg = (\n\u001b[32m    137\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    138\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    139\u001b[39m         )\n",
      "\u001b[31mRuntimeError\u001b[39m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0, 5.0], requires_grad=True) # Requires grad to make grad operations with the tensor\n",
    "y = torch.tensor([3.0, 7.0], requires_grad=True)\n",
    "\n",
    "\n",
    "z = x * y + y**2\n",
    "z.retain_grad() \n",
    "\n",
    "# Compute the gradients\n",
    "z_sum = z.sum().backward()\n",
    "\n",
    "\n",
    "print(f\"Gradient of x: {x.grad}\")\n",
    "print(f\"Gradient of y: {y.grad}\")\n",
    "print(f\"Gradient of z: {z.grad}\")\n",
    "print(f\"Result of the operation: z = {z.detach()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3dfa1b",
   "metadata": {},
   "source": [
    "$$\n",
    "z = xy + y^2 \\\\\n",
    "\\text{Let } p = xy,\\quad q = y^2 \\\\\n",
    "\\Rightarrow z = p + q\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\dfrac{\\partial z}{\\partial x} = \\dfrac{\\partial z}{\\partial p} \\cdot \\dfrac{\\partial p}{\\partial x} + \\dfrac{\\partial z}{\\partial q} \\cdot \\dfrac{\\partial q}{\\partial x} = y \\\\\n",
    "\\\\\n",
    "\\dfrac{\\partial z}{\\partial y} = \\dfrac{\\partial z}{\\partial p} \\cdot \\dfrac{\\partial p}{\\partial y} + \\dfrac{\\partial z}{\\partial q} \\cdot \\dfrac{\\partial q}{\\partial y} = x+2y\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7648eb",
   "metadata": {},
   "source": [
    "* By default, PyTorch only retains gradients for leaf tensors (i.e., tensors with requires_grad=True that are not the result of an operation). For non-leaf tensors (intermediate results in the computation graph), their .grad is not stored after .backward() unless you explicitly call .retain_grad() on them.\n",
    "\n",
    "* Calling .backward() on a scalar (like z_sum) triggers backpropagation. PyTorch computes the gradients of z_sum with respect to all tensors that have requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93e2424",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2.0], requires_grad=True)  # leaf tensor\n",
    "y = x * 3                                     # non-leaf tensor\n",
    "z = y ** 2\n",
    "\n",
    "y.retain_grad()  # Tell PyTorch to keep y's gradient\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(x.grad)  # Always available\n",
    "print(y.grad)  # Available only because of .retain_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbcb8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchviz import make_dot\n",
    "\n",
    "# dot  = make_dot(z, params={\"x\": x, \"y\": y, \"z\": z})\n",
    "# dot.render(\"grad_comp_graph\", format=\"png\")\n",
    "\n",
    "# img = plt.imread(\"grad_comp_graph.png\")\n",
    "# plt.imshow(img)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7e6d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
